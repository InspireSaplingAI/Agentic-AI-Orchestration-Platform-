{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Install dependencies ---\n",
    "%pip install langroid fastapi uvicorn pydantic[dotenv] langchain faiss-cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a4087",
   "metadata": {},
   "source": [
    "# Part 1 Agentic AI LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60110f0",
   "metadata": {},
   "source": [
    "## Example 1: Basic OpenAIGPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Cell 2: Single Langroid Agent (Hello World) ---\n",
    "import langroid as lr\n",
    "from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n",
    "from langroid.language_models.openai_gpt import OpenAIGPTConfig\n",
    "\n",
    "# configure the model (requires OPENAI_API_KEY in env or compatible endpoint)\n",
    "# llm_cfg = OpenAIGPTConfig(chat_model=\"gpt-3.5-turbo\", temperature=0.0) # chosose model here\n",
    "\n",
    "# if using local model, e.g., Llama2, use:\n",
    "# from langroid.language_models.local_llm import LocalLLMConfig \n",
    "# llm_cfg = LocalLLMConfig(model_path=\"/path/to/llama2/model\", temperature=0.0)\n",
    "\n",
    "# if using LM studio, use:\n",
    "from langroid.language_models.openai_gpt import OpenAIGPTConfig\n",
    "\n",
    "llm_cfg = OpenAIGPTConfig(\n",
    "    chat_model=\"local-llm\", \n",
    "    temperature=0.0,\n",
    "    api_base=\"http://localhost:1234/v1\",\n",
    "    api_key=\"not-needed\"   # dummy value, required by client but ignored by LM Studio\n",
    ")\n",
    "\n",
    "\n",
    "cfg = ChatAgentConfig(name=\"qa-agent\", llm=llm_cfg)\n",
    "agent = lr.agent.chat_agent.ChatAgent(cfg)\n",
    "\n",
    "# Example query\n",
    "response = agent.llm_response(\"Hello, what is an AI agent?\")\n",
    "print(\"Agent:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215c08b",
   "metadata": {},
   "source": [
    "## Example 1 Opional: Basic Chat with Assistant API\n",
    "Need openAPI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f31dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langroid.agent.openai_assistant import (\n",
    "    OpenAIAssistantConfig,\n",
    "    OpenAIAssistant,\n",
    "    AssistantTool,\n",
    ")\n",
    "from langroid.language_models.openai_gpt import OpenAIGPTConfig, OpenAIChatModel\n",
    "\n",
    "cfg = OpenAIAssistantConfig(\n",
    "    llm = OpenAIGPTConfig(chat_model=OpenAIChatModel.GPT4_TURBO)\n",
    ")\n",
    "agent = OpenAIAssistant(cfg)\n",
    "\n",
    "response = agent.llm_response(\"What is the square of 3?\")\n",
    "print(\"Agent:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d85e2",
   "metadata": {},
   "source": [
    "## Example 2: Wrap Agent in a Task, run it\n",
    "An OpenAIAssistant agent has various capabilities (LLM responses, agent methods/tools, etc) but there is no mechanism to iterate over these capabilities or with a human or with other agents. This is where the Task comes in: Wrapping this agent in a Task allows you to run interactive loops with a user or other agents (you will see more examples below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langroid.agent.task import Task\n",
    "\n",
    "task = Task(\n",
    "    agent,\n",
    "    system_message=\"\"\"User will give you a word,\n",
    "      return its antonym if possible, else say DO-NOT-KNOW.\n",
    "      Be concise!\",\n",
    "      \"\"\",\n",
    "    single_round=True\n",
    ")\n",
    "result = task.run(\"ignorant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e51c6",
   "metadata": {},
   "source": [
    "## Example 2 Optional: OpenAIAssistant Agent + Task with Code Interpreter\n",
    "Here we attach the \"code_interpreter\" tool (from the OpenAI Assistant API) to the agent defined above, and run it in a task.\n",
    "Need openAPI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f1917",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.add_assistant_tools([AssistantTool(type=\"code_interpreter\")])\n",
    "task = Task(agent, interactive=False, single_round=True)\n",
    "result = task.run(\"What is the 10th Fibonacci number, if you start with 1,2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b8c0a3",
   "metadata": {},
   "source": [
    "# Example 3: RAG\n",
    "Attach a file (a lease document) and the \"retrieval\" tool, and ask questions about the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11014fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langroid.language_models.openai_gpt import OpenAIGPTConfig, OpenAIGPT\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings  # can be replaced with local embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "import requests\n",
    "file_url = \"https://raw.githubusercontent.com/langroid/langroid-examples/main/examples/docqa/lease.txt\"\n",
    "response = requests.get(file_url)\n",
    "with open('lease.txt', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "    \n",
    "# 1️⃣ Load document\n",
    "with open(\"lease.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 2️⃣ Split into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "chunks = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "\n",
    "# ✅ Initialize HuggingFaceEmbeddings using model_name\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"}  # or \"cuda\" for GPU\n",
    ")\n",
    "\n",
    "vectordb = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 4️⃣ Setup local LLM\n",
    "llm_cfg = OpenAIGPTConfig(\n",
    "    chat_model=\"local-llm\",\n",
    "    temperature=0.0,\n",
    "    api_base=\"http://localhost:1234/v1\",\n",
    "    api_key=\"not-needed\"\n",
    ")\n",
    "llm = OpenAIGPT(llm_cfg)\n",
    "\n",
    "# 5️⃣ Retrieval function\n",
    "def retrieve_answer(query):\n",
    "    results = vectordb.similarity_search(query, k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in results])\n",
    "    prompt = f\"Answer the question based on the following document:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    response = llm.chat(prompt)\n",
    "    return response\n",
    "\n",
    "# 6️⃣ Ask the question\n",
    "answer = retrieve_answer(\"What is the start date of the lease?\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75cb1e",
   "metadata": {},
   "source": [
    "You can also use local embeddings or embeddings throught http request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8fc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:1234/v1/embeddings\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"text-embedding-nomic-embed-text-v1.5\",\n",
    "    \"input\": \"Hello LM Studio!\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Embedding response:\", response.json())\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7864d92",
   "metadata": {},
   "source": [
    "## Example 3 - Optional : using OpenAI AIAsisstant\n",
    "OpenAIAssistant with Retrieval\n",
    "Need openAPI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "file_url = \"https://raw.githubusercontent.com/langroid/langroid-examples/main/examples/docqa/lease.txt\"\n",
    "response = requests.get(file_url)\n",
    "with open('lease.txt', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# verify\n",
    "#with open('lease.txt', 'r') as file:\n",
    "#   print(file.read())\n",
    "\n",
    "# now create agent, add retrieval tool and file\n",
    "agent = OpenAIAssistant(cfg)\n",
    "agent.add_assistant_tools([AssistantTool(type=\"retrieval\")])\n",
    "agent.add_assistant_files([\"lease.txt\"])\n",
    "response = agent.llm_response(\"What is the start date of the lease?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f887d",
   "metadata": {},
   "source": [
    "## Example 4:  Task: Custom Function-calling\n",
    "You can define your own custom function (or ToolMessage in Langroid terminology), enable the agent to use it, and have a special method to handle the message when the LLM emits such a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langroid.language_models.openai_gpt import OpenAIGPTConfig, OpenAIGPT\n",
    "import re\n",
    "\n",
    "# Setup local LLM\n",
    "llm_cfg = OpenAIGPTConfig(\n",
    "    chat_model=\"local-llm\",\n",
    "    temperature=0.0,\n",
    "    api_base=\"http://localhost:1234/v1\",\n",
    "    api_key=\"not-needed\"\n",
    ")\n",
    "llm = OpenAIGPT(llm_cfg)\n",
    "\n",
    "# Define tools\n",
    "def square_tool(num: int) -> str:\n",
    "    return str(num ** 2)\n",
    "\n",
    "def cube_tool(num: int) -> str:\n",
    "    return str(num ** 3)\n",
    "\n",
    "tools = {\n",
    "    \"square\": square_tool,\n",
    "    \"cube\": cube_tool\n",
    "}\n",
    "\n",
    "# Function to ask LLM which tool to use\n",
    "def agent_response(user_input: str) -> str:\n",
    "    # Step 1: LLM decides which tool and input\n",
    "    prompt = f\"\"\"\n",
    "You are an assistant. The user asks: \"{user_input}\".\n",
    "You have two tools available:\n",
    "- square(number): returns the square of a number\n",
    "- cube(number): returns the cube of a number\n",
    "\n",
    "Respond ONLY like this:\n",
    "TOOL: <tool_name>, INPUT: <number>\n",
    "\"\"\"\n",
    "    llm_reply = llm.chat(prompt).message.strip()\n",
    "\n",
    "    # Step 2: Parse tool name and input number\n",
    "    tool_match = re.search(r\"TOOL:\\s*(\\w+)\", llm_reply)\n",
    "    num_match = re.search(r\"INPUT:\\s*(\\d+)\", llm_reply)\n",
    "\n",
    "    if tool_match and num_match:\n",
    "        tool_name = tool_match.group(1)\n",
    "        num = int(num_match.group(1))\n",
    "\n",
    "        # Call the selected tool\n",
    "        if tool_name in tools:\n",
    "            result = tools[tool_name](num)\n",
    "            return f\"LLM selected tool: {tool_name}\\nDONE: The result is {result}\"\n",
    "        else:\n",
    "            return f\"Unknown tool selected: {tool_name}\"\n",
    "    else:\n",
    "        return \"Could not determine the tool or input.\"\n",
    "\n",
    "# Run examples\n",
    "user_input1 = \"What is the square of 7?\"\n",
    "response1 = agent_response(user_input1)\n",
    "print(response1)\n",
    "\n",
    "user_input2 = \"What is the cube of 3?\"\n",
    "response2 = agent_response(user_input2)\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47e99d",
   "metadata": {},
   "source": [
    "## Example 4 Optional: OpenAIAsssistant + Task: Custom Function-calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own function for the LLM to call;\n",
    "# this function will be executed by the Langroid agent as part of the task loop\n",
    "\n",
    "class SquareTool(ToolMessage):\n",
    "    request = \"square\"\n",
    "    purpose = \"To find the square of a number \"\n",
    "    num: int\n",
    "\n",
    "    def handle(self) -> str:\n",
    "        return str(self.num ** 2)\n",
    "\n",
    "# create agent, add tool to agent\n",
    "cfg = OpenAIAssistantConfig(\n",
    "    llm=OpenAIGPTConfig(chat_model=OpenAIChatModel.GPT4_TURBO),\n",
    "    name=\"NumberExpert\",\n",
    ")\n",
    "agent = OpenAIAssistant(cfg)\n",
    "agent.enable_message(SquareTool)\n",
    "task = Task(\n",
    "    agent,\n",
    "    system_message=\"\"\"\n",
    "    User will ask you to square a number.\n",
    "    You do NOT know how, so you will use the\n",
    "    `square` function to find the answer.\n",
    "    When you get the answer say DONE and show it.\n",
    "    \"\"\",\n",
    "    interactive=False,\n",
    ")\n",
    "response = task.run(\"What is the square of 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d1c99",
   "metadata": {},
   "source": [
    "# Part 2 Wrap Agent with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c17238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Wrap with FastAPI (run in background) ---\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()  # allows uvicorn to run inside notebook\n",
    "\n",
    "app = FastAPI(title=\"Agentic AI Demo\")\n",
    "\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "\n",
    "# --- Updated RetrievalAgent with logging ---\n",
    "class RetrievalAgent:\n",
    "    def __init__(self, llm, vectordb):\n",
    "        self.llm = llm\n",
    "        self.vectordb = vectordb\n",
    "\n",
    "    def llm_response(self, query: str):\n",
    "        print(\"[STATUS] Received query:\", query)\n",
    "\n",
    "        # 1️⃣ Retrieve relevant chunks\n",
    "        print(\"[STATUS] Running vector search...\")\n",
    "        results = self.vectordb.similarity_search(query, k=3)\n",
    "        print(f\"[STATUS] Found {len(results)} relevant chunks\")\n",
    "\n",
    "        context = \"\\n\".join([doc.page_content for doc in results])\n",
    "        print(\"[STATUS] Preparing LLM prompt...\")\n",
    "\n",
    "        # 2️⃣ Send to local LLM\n",
    "        prompt = f\"Answer the question based on the following document:\\n{context}\\n\\nQuestion: {query}\"\n",
    "        print(\"[STATUS] Sending prompt to LLM...\")\n",
    "        response = self.llm.chat(prompt)\n",
    "        print(\"[STATUS] Received response from LLM\")\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "# Instantiate retrieval_agent\n",
    "retrieval_agent = RetrievalAgent(llm, vectordb)\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI(title=\"Agentic AI Demo\")\n",
    "\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "\n",
    "# --- Health check endpoint ---\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    # You can add extra info if you like, e.g., number of documents\n",
    "    num_docs = len(vectordb.docstore._dict) if hasattr(vectordb.docstore, \"_dict\") else \"unknown\"\n",
    "    return {\"status\": \"ok\", \"num_docs_in_vectordb\": num_docs}\n",
    "\n",
    "# --- Ask endpoint ---\n",
    "@app.post(\"/ask\")\n",
    "def ask_agent(q: Query):\n",
    "    try:\n",
    "        print(\"[FASTAPI] Received request:\", q.question)\n",
    "        response = retrieval_agent.llm_response(q.question)\n",
    "\n",
    "        # Use the correct attribute for LLM response text\n",
    "        answer = getattr(response, \"content\", None) or getattr(response, \"message\", None)\n",
    "        if not answer:\n",
    "            answer = str(response)  # fallback to string conversion\n",
    "\n",
    "        print(\"[FASTAPI] Returning response\")\n",
    "        return {\"answer\": answer}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[FASTAPI] Error:\", e)\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# --- Run Uvicorn in background ---\n",
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_app():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "thread = threading.Thread(target=run_app, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "print(\"FastAPI server started on http://127.0.0.1:8000\")\n",
    "print(\"Health check available at http://127.0.0.1:8000/health\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Test FastAPI endpoint ---\n",
    "import requests\n",
    "\n",
    "# Health check\n",
    "res = requests.get(\"http://127.0.0.1:8000/health\")\n",
    "print(res.json())\n",
    "\n",
    "\n",
    "# ask question\n",
    "payload = {\"question\": \"What info is given in the how the lease would end?\"}\n",
    "\n",
    "try:\n",
    "    # Add timeout to avoid hanging indefinitely\n",
    "    res = requests.post(\"http://127.0.0.1:8000/ask\", json=payload, timeout=300)\n",
    "    \n",
    "    print(\"Status code:\", res.status_code)\n",
    "    print(\"Raw response text:\", res.text)  # debug output\n",
    "    \n",
    "    # Try parsing JSON only if response is not empty\n",
    "    if res.text:\n",
    "        print(res.json())\n",
    "    else:\n",
    "        print(\"Empty response received\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"Request timed out. The LLM may be taking too long to respond.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Request failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clear session\n",
    "%reset -f   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e83ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
